global:
  runningOnAws: true
  cluster:
    name: ${cluster_name}
    domain: ${cluster_domain}
    domain_id: ${cluster_domain_id}
    egressIpAddresses: ${egress_ip_addresses}
    oidcProviderURL: ${cluster_oidc_provider_url}
    oidcProviderARN: ${cluster_oidc_provider_arn}
  account:
    name: ${account_name}
  roles:
    harbor: ${harbor_iam_role_name}
    concourse: ${concourse_iam_role_name}
  cloudHsm:
    enabled: false
    ip: "127.0.0.1"
  # move these to gsp-namespace terraform output
  canary:
    verificationKeys: []

adminRoleARNs: ${admin_role_arns}
adminUserARNs: ${admin_user_arns}
sreRoleARNs: ${sre_role_arns}
sreUserARNs: ${sre_user_arns}
devRoleARNs: []
bootstrapRoleARNs: ${bootstrap_role_arns}

permittedRolesRegex: ${permitted_roles_regex}

httpsEgressSafelist: []
httpEgressSafelist: []

notary:
  rootPassphrase: ${notary_root_passphrase}
  rootKey: ${notary_root_key}
  targetsPassphrase: ${notary_targets_passphrase}
  snapshotPassphrase: ${notary_snapshot_passphrase}
  delegationPassphrase: ${notary_delegation_passphrase}
  delegationKey: ${notary_delegation_key}

cluster-autoscaler:
  cloudProvider: aws
  awsRegion: eu-west-2
  autoDiscovery:
    clusterName: ${cluster_name}

concourseMainTeamGithubTeams: ${concourse_main_team_github_teams}
concourse:
  secrets:
    localUsers: >-
      pipeline-operator:${concourse_admin_password}
    githubClientId: ${github_client_id}
    githubClientSecret: ${github_client_secret}
    githubCaCert: ${github_ca_cert}
  worker:
    replicas: ${concourse_worker_count}
    nodeSelector:
      node-role.kubernetes.io/ci: ""
    annotations:
      iam.amazonaws.com/role: ${concourse_iam_role_name}
    tolerations:
      - key: "node-role.kubernetes.io/ci"
        operator: Exists
        effect: NoSchedule
  concourse:
    web:
      externalUrl: https://ci.${cluster_domain}
      auth:
        github:
          enabled: true
        mainTeam:
          localUser: pipeline-operator
          config: /web-configuration/config.yaml
      kubernetes:
        namespacePrefix: ${cluster_name}-
        createTeamNamespaces: false
        teams: ${concourse_teams}

pipelineOperator:
  concourseUsername: pipeline-operator
  concoursePassword: >-
    ${concourse_admin_password}

harbor:
  harborAdminPassword: ${harbor_admin_password}
  secretKey: ${harbor_secret_key}
  externalURL: https://registry.${cluster_domain}
  expose:
    ingress:
      hosts:
        core: registry.${cluster_domain}
        notary: notary.${cluster_domain}
  persistence:
    imageChartStorage:
      type: s3
      s3:
        bucket: ${harbor_bucket_id}
        region: ${harbor_bucket_region}
        regionendpoint: s3.${harbor_bucket_region}.amazonaws.com
  core:
    secret: ${harbor_secret_key}
  jobservice:
    secret: ${harbor_secret_key}
  registry:
    secret: ${harbor_secret_key}
    podAnnotations:
      iam.amazonaws.com/role: ${harbor_iam_role_name}
  chartmuseum:
    podAnnotations:
      iam.amazonaws.com/role: ${harbor_iam_role_name}
  notary:
    secretName: gsp-harbor-notary-tls
    tlsCA: ${notary_ca_pem}
    tlsCert: ${notary_cert_pem}
    tlsKey: ${notary_root_key}
  database:
    type: external
    external:
      host: "${harbor_db_host}"
      port: "${harbor_db_port}"
      username: "${harbor_db_username}"
      password: "${harbor_db_password}"
      coreDatabase: "registry"
      clairDatabase: "clair"
      notaryServerDatabase: "notary_server"
      notarySignerDatabase: "notary_signer"
      sslmode: "require"

secrets:
  public_certificate: ${sealed_secrets_public_cert}
  private_key: ${sealed_secrets_private_key}

kiam:
  server:
    tlsFiles:
      ca: ${kiam_ca_cert_b64e_pem}
      cert: ${kiam_server_cert_b64e_pem}
      key: ${kiam_server_key_b64e_pem}
    assumeRoleArn: ${kiam_server_role_arn}
    roleBaseArn: "arn:aws:iam::${account_id}:role/"
    updateStrategy: RollingUpdate
  agent:
    tlsFiles:
      ca: ${kiam_ca_cert_b64e_pem}
      cert: ${kiam_agent_cert_b64e_pem}
      key: ${kiam_agent_key_b64e_pem}
    host:
      interface: "eni+"
    updateStrategy: RollingUpdate

fluentd-cloudwatch:
  extraVars:
    - "{ name: CLUSTER_NAME, value: ${cluster_name} }"
  logGroupName: ${cloudwatch_log_group_name}
  awsRole: ${cloudwatch_log_shipping_role}
  tolerations:
  - operator: Exists
    effect: NoSchedule

gsp-monitoring:
  prometheus-operator:
    kubeTargetVersionOverride: "${eks_version}"
    prometheus:
      prometheusSpec:
        externalLabels:
          clustername: ${cluster_domain}
          product: ${account_name}
        additionalAlertManagerConfigs:
        - static_configs:
          - targets:
            - "alerts-1.monitoring.gds-reliability.engineering"
            - "alerts-2.monitoring.gds-reliability.engineering"
            - "alerts-3.monitoring.gds-reliability.engineering"
          scheme: https
    grafana:
      podAnnotations:
        eks.amazonaws.com/role-arn: arn:aws:iam::${account_id}:role/${grafana_iam_role_name}
      adminPassword: ${grafana_default_admin_password}
      grafana.ini:
        server:
          root_url: https://grafana.${cluster_domain}
        users:
          viewers_can_edit: true
          auto_assign_org: true
          auto_assign_org_role: Editor

serviceOperator:
  kiamServerRoleARN: ${kiam_server_role_arn}
  permissionsBoundaryARN: ${service_operator_boundary_arn}

RDSFromWorkerSecurityGroup: ${rds_from_worker_security_group}
privateDBSubnetGroup: ${private_db_subnet_group}

externalDns:
  iamRoleName: ${external_dns_iam_role_name}

cert-manager:
  podAnnotations:
    iam.amazonaws.com/role: ${cert_manager_role_name}
