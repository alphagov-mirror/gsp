global:
  runningOnAws: true
  cluster:
    name: ${cluster_name}
    domain: ${cluster_domain}
    domain_id: ${cluster_domain_id}
    egressIpAddresses: ${egress_ip_addresses}
    oidcProviderURL: ${cluster_oidc_provider_url}
    oidcProviderARN: ${cluster_oidc_provider_arn}
  account:
    name: ${account_name}
    id: ${account_id}
  cloudHsm:
    public: false
    enabled: false
    ip: "127.0.0.1"
  # move these to gsp-namespace terraform output
  canary:
    verificationKeys: []
  dockerHubCredentials: ${dockerhub_credentials}
  dockerHubUsername: ${dockerhub_username}
  dockerHubPassword: ${dockerhub_password}

adminRoleARNs: ${admin_role_arns}
devRoleARNs: []
bootstrapRoleARNs: ${bootstrap_role_arns}

httpsEgressSafelist: []
httpEgressSafelist: []

cluster-autoscaler:
  image:
    tag: v1.15.5
  cloudProvider: aws
  awsRegion: eu-west-2
  autoDiscovery:
    clusterName: ${cluster_name}

concourseMainTeamGithubTeams: ${concourse_main_team_github_teams}
concourse:
  secrets:
    localUsers: >-
      pipeline-operator:${concourse_admin_password}
    githubClientId: ${github_client_id}
    githubClientSecret: ${github_client_secret}
    githubCaCert: ${github_ca_cert}
  concourse:
    web:
      externalUrl: https://ci.${cluster_domain}
      auth:
        github:
          enabled: true
        mainTeam:
          config: |
            roles:
            - name: owner
              local:
                users: ["pipeline-operator"]
            - name: pipeline-operator
              github:
                teams: ${concourse_main_team_github_teams}
          localUser: pipeline-operator
      kubernetes:
        namespacePrefix: ${cluster_name}-
        createTeamNamespaces: false
        teams: ${concourse_teams}

pipelineOperator:
  concourseUsername: pipeline-operator
  concoursePassword: >-
    ${concourse_admin_password}

secrets:
  public_certificate: ${sealed_secrets_public_cert}
  private_key: ${sealed_secrets_private_key}

fluentd-cloudwatch:
  extraVars:
    - "{ name: CLUSTER_NAME, value: ${cluster_name} }"
  rbac:
    serviceAccountAnnotations:
      eks.amazonaws.com/role-arn: ${cloudwatch_log_shipping_role}
  tolerations:
  - operator: Exists
    effect: NoSchedule

gsp-monitoring:
  prometheus-operator:
    kubeTargetVersionOverride: "${eks_version}"
    prometheus:
      prometheusSpec:
        externalLabels:
          clustername: ${cluster_domain}
          product: ${account_name}
        additionalAlertManagerConfigs:
        - static_configs:
          - targets:
            - alerts-eu-west-1a.monitoring.gds-reliability.engineering
            - alerts-eu-west-1b.monitoring.gds-reliability.engineering
            - alerts-eu-west-1c.monitoring.gds-reliability.engineering
          scheme: https
    grafana:
      adminPassword: ${grafana_default_admin_password}
      grafana.ini:
        server:
          root_url: https://grafana.${cluster_domain}
        users:
          viewers_can_edit: true
          auto_assign_org: true
          auto_assign_org_role: Editor

serviceOperator:
  roleARN: ${service_operator_role_arn}
  permissionsBoundaryARN: ${service_operator_boundary_arn}

RDSFromWorkerSecurityGroup: ${rds_from_worker_security_group}
privateDBSubnetGroup: ${private_db_subnet_group}

redisFromWorkerSecurityGroup: ${redis_from_worker_security_group}
privateRedisSubnetGroup: ${private_redis_subnet_group}

${external_dns_map}

cert-manager:
  serviceAccount:
    annotations:
      eks.amazonaws.com/role-arn: ${cert_manager_role_arn}
